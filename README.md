# Uber Data Analytics ETL Pipeline

A robust ETL (Extract, Transform, Load) pipeline for processing Uber ride booking data from Kaggle, featuring data validation with Pydantic and cloud storage integration.

## ğŸ“Š Project Overview

This project implements a complete data pipeline that:
- Extracts Uber ride booking data from Kaggle
- Transforms and validates data using Pydantic models
- Loads processed data to S3-compatible storage (LocalStack for development)

**Data Source**: [Uber Ride Analytics Dashboard](https://www.kaggle.com/datasets/yashdevladdha/uber-ride-analytics-dashboard)

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Kaggle    â”‚â”€â”€â”€â–¶â”‚   Transform  â”‚â”€â”€â”€â–¶â”‚   S3 Bucket â”‚
â”‚  (Extract)  â”‚    â”‚  & Validate  â”‚    â”‚   (Load)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Pipeline Components

1. **Extract** ([`controllers/extract.py`](uber_data_analytics/controllers/extract.py))
   - Downloads NCR ride bookings CSV from Kaggle using `kagglehub`
   - Handles dataset caching and file management

2. **Transform** ([`controllers/transform.py`](uber_data_analytics/controllers/transform.py))
   - Processes CSV data with pandas
   - Validates booking records using Pydantic models
   - Converts data to JSON format

3. **Load** ([`services/storage_service.py`](uber_data_analytics/services/storage_service.py))
   - Uploads processed data to S3-compatible storage
   - Supports both AWS S3 and LocalStack

## ğŸ“‹ Data Schema

The pipeline processes booking data with the following structure (defined in [`controllers/bookings_schema.py`](uber_data_analytics/controllers/bookings_schema.py)):

| Field | Type | Description |
|-------|------|-------------|
| `date` | string | Booking date (YYYY-MM-DD) |
| `time` | string | Booking time (HH:MM:SS) |
| `booking_id` | string | Unique booking identifier |
| `booking_status` | string | Status (completed/cancelled/incomplete) |
| `customer_id` | string | Customer identifier |
| `vehicle_type` | string | Vehicle type (sedan/suv/hatchback) |
| `pickup_location` | string | Pickup location |
| `drop_location` | string | Drop-off location |
| `avg_vtat` | float | Average Vehicle Time to Arrival (minutes) |
| `avg_ctat` | float | Average Customer Time to Arrival (minutes) |
| `booking_value` | float | Monetary value of booking |
| `ride_distance` | float | Distance in kilometers |
| `driver_ratings` | float | Driver rating |
| `customer_rating` | float | Customer rating |
| `payment_method` | string | Payment method used |

### Computed Fields
- `datetime`: Combined date/time as ISO datetime
- `total_wait_time`: Sum of VTAT and CTAT

## ğŸš€ Getting Started

### Prerequisites

- Python 3.13+
- Kaggle API credentials
- Docker (for LocalStack)

### Installation

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd uber-data-analysis
   ```

2. **Install dependencies**
   ```bash
   # Using uv (recommended)
   uv sync
   ```

3. **Set up Kaggle API**
   ```bash
   # Create kaggle directory
   mkdir ~/.kaggle

   # Add your kaggle.json credentials file
   cp kaggle.json ~/.kaggle/
   chmod 600 ~/.kaggle/kaggle.json
   ```

4. **Configure environment**
   ```bash
   cp .env.example .env
   # Edit .env with your settings
   ```


### Local Development with LocalStack

1. **Start LocalStack**
   ```bash
   sudo localstack start
   ```

2. **Create S3 bucket**
   ```bash
   awslocal s3 mb s3://my-bucket
   ```

## ğŸƒâ€â™‚ï¸ Usage

### Run the Complete Pipeline

```bash
# Using uv
uv run python -m uber_data_analytics.main

# Or directly
python -m uber_data_analytics.main
```

## ğŸ“ Project Structure

```
uber-data-analysis/
â”œâ”€â”€ uber_data_analytics/
â”‚   â”œâ”€â”€ controllers/           # Data processing logic
â”‚   â”‚   â”œâ”€â”€ extract.py        # Kaggle data extraction
â”‚   â”‚   â”œâ”€â”€ transform.py      # Data transformation
â”‚   â”‚   â””â”€â”€ bookings_schema.py # Pydantic data models
â”‚   â”œâ”€â”€ services/             # External service integrations
â”‚   â”‚   â””â”€â”€ storage_service.py # S3 storage operations
â”‚   â”œâ”€â”€ data/                 # Data files (gitignored)
â”‚   â”œâ”€â”€ main.py              # Main pipeline orchestrator
â”‚   â”œâ”€â”€ settings.py          # Configuration management
â”‚   â””â”€â”€ resources.py         # Dependency injection
â”œâ”€â”€ tests/                   # Test suite
â”œâ”€â”€ notebooks/              # Jupyter notebooks for exploration
â”œâ”€â”€ .github/workflows/      # CI/CD pipelines
â””â”€â”€ pyproject.toml         # Project configuration
```
